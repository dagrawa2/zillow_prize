\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, graphicx}
\usepackage{hyperref}

\title{Predicting the error in Zillow Zestimates}
\author{Devanshu Agrawal}
\date{}

\begin{document}
\maketitle

\section{Introduction and problem statement}

\section{Data exploration and preprocessing}

We were provided with four data sets: properties\_2016, transactions\_2016, properties\_2017, and transactions\_2017. We will discuss properties\_2016 and properties\_2017 first. The set properties\_2016 contains 2985217 properties along with 57 features that describe location, total lot area, number of rooms, etc. The set properties\_2017 contains the same 2985217 properties and 57 features but includes information updated to 2017. Both data sets are missing a significant fraction of their values, and many features are redundant. We therefore proceed to discuss all 57 features in some detail and describe which features we decided to discard and what strategies we employed to impute missing values. Note that the discussion is very similar for both properties\_2016 and properties\_2017. We therefore discuss only properties\_2016 for brevity. Note also that our preprocessing as described below borrows heavily from \cite{rtlatimer}.

\subsection{Preprocessing properties data}
\label{section-preprocess}

We start by computing the fraction of missing values in each of the 57 features in properties\_2016 (see Table \ref{table-miss}). All features have some number of missing values, and many features are missing over 99\% of their values. We found that there are primarily two types of missing values:
\begin{enumerate}
\item Some values are missing to imply a null value such as a pool size of $0$ when a property has no pool.
\item Other values are missing for unknown reasons.
\end{enumerate}
We now proceed to go through the features one-by-one and describe our methods of imputation. We will note define what each feature measures; for this information, we refer the reader to the feature dictionary at \cite{kaggle}.

\begin{table}
\centering
\caption{\label{table-miss} Fraction of values missing for each feature in properties\_2016 sorted in descending order.}
\begin{tabular}{|c|c|c|} \hline
Rank & Feature & Fraction missing \\ \hline
1 & storytypeid & 0.9995 \\
2 & basementsqft & 0.9995 \\
3 & yardbuildingsqft26 & 0.9991 \\
4 & fireplaceflag & 0.9983 \\
5 & architecturalstyletypeid & 0.998 \\
6 & typeconstructiontypeid & 0.9977 \\
7 & finishedsquarefeet13 & 0.9974 \\
8 & buildingclasstypeid & 0.9957 \\
9 & decktypeid & 0.9943 \\
10 & finishedsquarefeet6 & 0.9926 \\
\vdots & \vdots & \vdots \\
26 & airconditioningtypeid & 0.7282 \\
27 & garagecarcnt & 0.7041 \\
28 & garagetotalsqft & 0.7041 \\
29 & regionidneighborhood & 0.6126 \\
30 & heatingorsystemtypeid & 0.3949 \\
\vdots & \vdots & \vdots \\
53 & fips & 0.0038 \\
54 & regionidcounty & 0.0038 \\
55 & latitude & 0.0038 \\
56 & longitude & 0.0038 \\
57 & rawcensustractandblock & 0.0038 \\
\hline
\end{tabular}
\end{table}

Consider the features ``yearbuilt'', ``architecturalstyletypeid'', ``buildingclasstypeid'', ``typeconstructiontypeid'', ``finishedsquarefeet13'', ``decktypeid'', ``storytypeid'', and ``numberofstories''. We imputed ``yearbuilt'' with its mode. It appears that the value of ``decktypeid'' is either 66 or is missing. We therefore replace 66 with $1$ as in ``deck is present'' and impute missing values with $0$ as in ``no deck''. Missing values of ``numberofstories'' probably correspond to properties with only one story; we impute this feature accordingly. All other features listed above are missing over 99\% of their values, and it is not obvious how these missing values could be imputed. We therefore drop these remaining features from the data.

Consider the features ``unitcnt'', ``buildingtypeid'', ``roomcnt'', and ``bedroomcnt''. We assume that missing values of ``unitcnt'' correspond to $1$, and we impute accordingly. We impute the other three features with their respective modes.

The features ``regionidcity'', ``regionidcounty'', ``regionidzip'', ``regionidneighborhood'', ``latitude'', and ``longitude'' describe the location of a property. We found that whenever ``latitude'' or ``longitude'' is missing, then all other location features are missing as well. We imputed ``latitude'' and ``longitude'' with their modes; we did not use mean or median as these could result in coordinates that correspond to a non-residential area. We then used the python package ``uszipcode'' to look up all missing zipcodes based on the now known coordinates. To impute missing counties, we took the following strategy: We first observed that ``regionidcounty'' contained three counties, and we assumed that each missing county was one of these three. Then for each property with missing county, we found another property in the data with the same zipcode but with known county; we could then impute the county of the former property with that of the latter. Fortunately, all zipcodes had at least one property with known county. We took a similar approach to impute missing values of ``regionidcity''. But we found one zipcode in the data -- 97331 -- for which all properties had missing city information. We used uszipcode to find that the city corresponding to zipcode 97331 is Corvallis, OR. Unfortunately, we could not find the Zillow city ID for Corvallis. Since the City ID is somewhat arbitrary anyways, we decided to impute the missing city ID with the floored mean of all other IDs in ``regionidcity''; we made sure that the imputed value did not already exist in the data. Finally, we were unable to impute ``regionidneighborhood'' as uszipcode cannot return the Zillow ID of a neighborhood. Since over 60\% of ``regionidneighborhood'' is missing anyway, we decided to drop this feature.

The features ``poolcnt'', ``poolsizzesum'', ``hashottuborspa'', ``pooltypeid2'', ``pooltypeid7'', and ``pooltypeid10'' pertain to information about pools. We first dropped ``pooltypeid10'' as it is identical to ``hashottuborspa''. The features ``hashottuborspa'', ``pooltypeid2'', and ``pooltypeid7'' are binary with either a value of $1$ (or ``True'') or missing a value. We therefore converted all ``True'' entries to $1$ and imputed missing values in these features with $0$ (as in ``False''). Similarly, we assumed that all missing values in ``poolcnt'' correspond to zero pools and imputed accordingly. We also assumed that for properties with no pools, missing values of ``poolsizesum'' indicate a pool size of $0$; we imputed these accordingly. For properties that do have a pool, we imputed missing values of ``poolsizesum'' with the median of all other pool sizes.

The features ``bathroomcnt'', ``calculatedbathnbr'', ``fullbathcnt'', and ``threequarterbathnbr'' pertain to bathroom information. We found that the following relationship holds:
\[ (\mbox{calculatedbathnbr}) = (\mbox{fullbathcnt}) + 0.5(\mbox{threequarterbathnbr}). \]
This suggests that the three-quarter bathrooms are in fact half bathrooms. Assuming this relationship holds even when missing values are present, we deduced that the missing values of ``threequarterbathnbr'' correspond to properties with no three-quarter bathrooms; we therefore imputed these missing values with $0$. Due to the colinearity described by the above relationship, we also decided to drop ``fullbathcnt''. We found also that ``bathroomcnt'' and ``calculatedbathnbr'' are equal whenever both values are available for a property. Since ``calculatedbathnbr'' has more missing values, we dropped it and kept ``bathroomcnt''.

The features ``garagecarcnt'' and ``garagetotalsqft'' give garage information. Missing values of ``garagecarcnt'' appear to correspond to properties with no garages; we imputed these with $0$. For these same properties with no garages, ``garagetotalsqft'' is also obviously missing; we imputed these values with $0$ as well.

The features ``finishedsquarefeet6'', ``finishedsquarefeet15'', ``finishedfloor1squarefeet'', ``finishedsquarefeet50'', ``finishedsquarefeet12'', ``calculatedfinishedsquarefeet'' provide information about finished area on a property. The features ``finishedsquarefeet15'', ``finishedsquarefeet12'', and ``calculatedfinishedsquarefeet'' appear identical; at the very least, we confirmed their colinearity by finding their pairwise Pearson correlations to be $1$. We therefore dropped ``finishedsquarefeet12'' and ``finishedsquarefeet15''. It is unclear what ``finishedsquarefeet6'' measures. Moreover, it is missing over 99\% of its values, and whenever a value is provided, it is identical to ``calculatedfinishedsquarefeet''. We hence drop ``finishedsquarefeet6'' as well. The features ``finishedfloor1squarefeet'' and ``finishedsqquarefeet50'' appear identical, and so we dropped the former. We impute the missing values of ``calculatedfinishedsquarefeet'' with the mean of its values. The feature ``finishedsquarefeet50'' gives the area of the first floor. For properties with one story, ``finishedsquarefeet50'' should be equal to the total finished area ``calculatedfinishedsquarefeet''. Therefore, for properties with one story, we impute ``finishedsquarefeet50'' with ``calculatedfinishedsquarefeet''. Finally, we impute all remaining missing values of ``finishedsquarefeet50'' with the mean of its values.

The features ``airconditioningtypeid'' and ``heatingorsystemtypeid'' describe the AC and heating units on the property. It appears that values are not entered whenever there is no AC or no heating unit. We therefore impute ``airconditioningtypeid'' with 5, which is the ID corresponding to ``None'', and similarly impute ``heatingorsystemtypeid'' with 13, which is the ID for ``None''.

The features ``fireplaceflag'' and ``fireplacecnt'' describe the presence and number of fireplaces on a property. We convert all ``True'' and ``False'' entries in ``fireplaceflag'' to $1$ and $0$ respectively. For properties with a fireplace (``fireplaceflag'' is $1$), we impute missing values of ``fireplacecnt'' with the median of all positive fireplace counts. For properties with no fireplace (``fireplaceflag'' is $0$), we impute ``fireplacecnt'' with $0$. Finally, we impute missing values of ``fireplaceflag'' to $0$ if the corresponding ``fireplacecnt'' is $0$ and impute to $1$ otherwise.

The features ``landtaxvaluedollarcnt'', ``structuretaxvaluedollarcnt'', ``taxvaluedollarcnt'', ``taxamount'', ``taxdelinquencyflag'', ``taxdelinquencyyear'', and ``assessmentyear'' pertain to taxes and tax delinquency. We interpret missing values in ``landtaxvaluedollarcnt'' as indicating that the property has no land outside the structure-- e.g., an apartment in a large building. We interpret missing values in ``structuretaxvaluedollarcnt'' as indicating that the property has no structure-- i.e., an empty lot. We therefore impute the missing values in these two features with $0$. We impute the missing values in ``taxvaluedollarcnt'' simply with its mean. Instead of imputing ``taxamount'', we form a new feature ``taxpercentage'' obtained as the ratio of ``taxamount'' and ``taxvaluedollarcnt''. We then drop ``taxamount'' and impute missing values in ``taxpercentage'' with its mean. The feature ``taxdelinquencyflag'' appears to be filled only when the flag is present. We therefore impute missing values with $0$ and convert all affirmative values to $1$. Since over 98\% of ``taxdelinquencyyear'' is missing, then we drop it. We imputed the missing values of ``assessmentyear'' with its mode.

The features ``lotsizesquarefeet'', ``yardbuildingsqft17'', ``yardbuildingsqft26'', and ``basementsqft'' give additional information about property area. We impute missing values in ``lotsizesquarefeet'' with its mean. Missing values in the other features suggest an area of $0$, and so we impute these accordingly.

Now we consider the features ``propertycountylandusecode'', ``propertylandusetypeid'', ``propertyzoningdesc'', ``fips'', ``rawcensustractandblock'', and ``censustractandblock''. The features ``propertycountylandusecode'' and ``propertyzoningdesc'' have entries that cannot be converted to a numerical representation. One option would be to treat these as categorical variables. But ``propertycountylandusecode'' and ``propertyzoningdesc'' have 240 and 5638 distinct values respectively; such large numbers of categories could be detrimental to our model. So instead, we decided to drop these two features. We imputed ``propertylandusetypeid'' with its mode. We found the features ``fips'', ``rawcensustractandblock'', and ``censustractandblock'' to have pairwise Pearson correlations very close to $1$. We therefore decided to drop the latter two.

This completes the preprocessing of properties\_2016. We refer to this preprocessed properties data set as properties\_2016\_preprocessed. The set properties\_2016\_preprocessed has 39 features.

\subsection{Building the training and test sets}
\label{section-transactions}

Now we turn to the data set transactions\_2016. This data set contains the subset of properties in properties\_2016\_preprocessed that were sold in 2016. The data set contains 90275 transactions (rows) but only 90150 unique properties, suggesting that some properties were sold more than once in 2016. For each property in transactions\_2016, two features are recorded-- the transaction date and the log-error between the actual price for which the property was sold and the Zestimate. The data has no missing values. We parsed the transaction dates and kept only the month while discarding year and day. To gain some intuition, we plotted the number of transactions over month (see Figure \ref{fig-month}). Most transactions occur during the summer. There are notably fewer transactions in October, November, and December; this is in part because Kaggle reserves a subset of transactions in these months to form the test set on which our predictions are to be evaluated.

\begin{figure}
\centering
\includegraphics[width=3in]{../results/explore/2016/plots/transactions.png}
\caption{\label{fig-month} Number of transactions in each month of 2016 as provided in the training set. Transactions are fewer in October, November, and December since a fraction of these are reserved for the test set.}
\end{figure}

We formed the training set for our regression models as follows: We took the subset of properties from properties\_2016\_preprocessed that were also listed in transactions\_2016; thus, we have 90275 not necessarily unique properties each with 38 features. We then augmented this data set with a new feature-- month, which we obtained from transactions\_2016. We observed that all properties in this set have an ``assessmentyear'' of 2015; the feature ``assessmentyear'' therefore provides no useful information for training, and hence we dropped this feature. We refer to the resulting data set as x\_train\_2016, which has 90275 not necessarily unique properties and 39 features. We then take the properties in transactions\_2016 with the log-error feature to form y\_train\_2016. The data (x\_train\_2016, y\_train\_2016) is then our training set for predicting log-error for each property in 2016.

The challenge is to predict log-error for all properties in properties\_2016\_preprocessed. We therefore build the test set as follows: We first take properties\_2016\_preprocessed and augment it with a ``month'' feature whose value is set to 10 (as in October); we call this augmented set x\_test\_2016\_oct. We construct x\_test\_2016\_nov and x\_test\_2016\_dec by setting the ``month'' feature to 11 and 12 respectively. Then to make predictions for October 2016, we apply our trained model on x\_test\_oct; we do similar for November and December. We let x\_test\_2016 be shorthand for the concatenated set (x\_test\_2016\_oct, x\_test\_2016\_nov, x\_test\_2016\_dec).

We build the training set (x\_train\_2017, y\_train\_2017) and test set x\_test\_2017 for 2017 in exactly the same way as we did for 2016.

\subsection{Correlations}

To gain further insight into the data, we computed correlations. We first computed the Pearson and Spearman correlation coefficients between each of the 39 features in x\_train\_2016 and the target log-error in y\_train\_2016. We plotted the absolute values of both correlation coefficients in a single coordinate plane (see Figure \ref{fig-corrs-with-y}). We see that not only are the Pearson correlations very small (absolute values less than $0.04$) but so are the Spearman correlations (absolute values less than $0.07$). This at first suggests that the dependence of log-error on each feature is more complex than a monotone relationship. In hindsight, however, we believe that the reason for such correlations is that log-error depends on multiple features. Thus, a plot of the log-error over a single feature would be full of noise due to the variation of the log-error with all other unseen features; see, for example, the plot of log-error over ``calculatedfinishedsquarefeet''-- the feature most strongly correlated with log-error (Figure \ref{fig-cs}). We believe that if we instead compute the correlation between a feature and log-error with all other features constrained to values in a small bin, then the mean of these correlations over all bins would be a more accurate measure of correlation between a single feature and log-error.

\begin{figure}
\centering
\includegraphics[width=3in]{../results/explore/2016/plots/corrs_with_y.png}
\caption{\label{fig-corrs-with-y} Plot of the absolute values of the Pearson and Spearman correlation coefficients between each feature in x\_train\_2016 and log-error. All correlations are very poor.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{../results/explore/2016/plots/cs-calculatedfinishedsquarefeet.png}
\caption{\label{fig-cs} Plot of log-error over ``calculatedfinishedsquarefeet''. The noise is likely due to the variation of log-error with the other 38 features not shown.}
\end{figure}

We also computed Pearson and Spearman correlation coefficients for every pair of the 39 features in x\_train\_2016. We plotted the absolute values of these correlations in a single coordinate plane (Figure \ref{fig-corrs-x}). Many features are strongly correlated with one another. Table \ref{table-corrs-x} lists the most positive and most negative Pearson correlations. We observe strong correlation between features that fall into natural groups; e.g., features pertaining to pools are correlated, and features pertaining to taxes are correlated. It is also interesting that many features correlate negatively with ``regionidcounty''. The data contains only three counties, but these counties are good predictors for some other features.

\begin{table}
\centering
\caption{\label{table-corrs-x} Pearson correlation coefficients between every pair of features sorted in descending order.}
\begin{tabular}{|c|c|c|c|} \hline
Rank & Feature 1 & Feature 2 & Pearson Correlation \\ \hline
1 & poolcnt & poolsizesum & 0.9966 \\
2 & poolcnt & pooltypeid7 & 0.9579 \\
3 & taxvaluedollarcnt & landtaxvaluedollarcnt & 0.9553 \\
4 & poolsizesum & pooltypeid7 & 0.9549 \\
5 & calculatedfinishedsquarefeet & finishedsquarefeet50 & 0.9405 \\
6 & fireplacecnt & fireplaceflag & 0.919 \\
7 & garagecarcnt & garagetotalsqft & 0.8398 \\
8 & structuretaxvaluedollarcnt & taxvaluedollarcnt & 0.8242 \\
\vdots & \vdots & \vdots & \vdots \\
737 & fips & fireplaceflag & 0.919 \\
738 & regionidcounty & garagetotalsqft & 0.8398 \\
739 & garagetotalsqft & taxvaluedollarcnt & 0.8242 \\
740 & heatingorsystemtypeid & calculatedfinishedsquarefeet & 0.7569 \\
741 & garagecarcnt & finishedsquarefeet50 & 0.7293 \\
\hline
\end{tabular}
\end{table}


\section{Dimensionality reduction}
\label{section-pca}

We were interested to see if compressing the data to only its most important dimensions could result in a more robust regression model. For this purpose, we prepared a compressed version of the data set. We applied principal components analysis (PCA) to identify the most important dimensions in the training data, and we then projected our data onto a fraction of these dimensions. More specifically, to first identify the best compression dimension, we standardized each column of the training data x\_train\_2016 by subtracting its mean and dividing by its standard deviation. We call the standardized training data x\_train\_2016\_standard. We then plotted the fraction of variance in x\_train\_2016\_standard explained by its first $k$ principal components (Figure \ref{fig-evr}). The first 25 principal components explain95\% of variance, and the first 31 principal components explain 99\% of variance. We decided to compress the data to 25 dimensions since we thought compressing from 39 dimensions to 31 was not extreme enough. To do this, we projected x\_train\_2016\_standard onto its first 25 principal components. We call the resulting data x\_train\_2016\_pca; this data set has 25 features.

\begin{figure}
\centering
\includegraphics[width=3in]{../results/explore/2016/plots/evr.png}
\caption{\label{fig-evr} Fraction of variance explained by the first $k$ principal components of x\_train\_2016\_standard. The first 25 principal components account for over 95\% of variance.}
\end{figure}

To compress the test set, we first standardized x\_test\_2016\_oct using the means and standard deviations of the training set x\_train\_2016, and we then projected it onto the first 25 principal components of x\_train\_2016\_standard. We call the resulting data set x\_test\_2016\_oct\_pca. We obtained the compressed data sets x\_test\_2016\_nov\_pca and x\_test\_2016\_dec\_pca in a similar manner. We let x\_test\_2016\_pca refer to the concatenation of the three per-month compressed test sets. The compressed test set also has 25 features.


\section{Regression Models}

Our task is to predict log-error in Zestimates on the test sets x\_test\_2016 and x\_test\_2017 given the training sets (x\_train\_2016, y\_train\_2016) and (x\_train\_2017, y\_train\_2017). We consider two tree-based regression models-- the random forest (RF) and the gradient-boosted decision tree (GBDT) regressors. An RF is an ensemble of decision trees where a prediction is taken to be the average of individual predictions made by all decision trees. Each decision tree base learner is trained on a random fraction of features and on a random bootstrapped sample of training data; the latter sampling method is called bootstrapped aggregation or ``bagging''. These sampling procedures help to reduce correlation between the base learners. The result is an ensemble model with both reduced bias and variance.

A GBDT improves upon a vanilla decision tree through the following iterative procedure: The model starts as a decision tree fit to the training data. A second decision tree is then fit to the gradient of the loss function evaluated at the predictions of the first decision tree. The tree-estimated gradient is then used to update or ``boost'' the first decision tree through one step of gradient descent; the result is a weighted sum of two decision trees. This model can then be boosted again through additional steps of gradient descent. Each tree in the model is trained on a fraction of features and bagging is also an option.

Both RFs and GBDTs are powerful and flexible models for regression tasks. They offer a range of hyperparameters that can be tuned to find a satisfactory balance between bias and variance. Both models are ubiquitous in Kaggle competitions with GBDTs often producing winning scores. This is likely because GBDTs are simpler to train, although they are more prone to overfitting than are RFs since the constituant trees in a GBDT are correlated.

We used the python API to the LightGBM library to implement both RFs and GBDTs. LightGBM is similar to the well-known XGBoost library and has been recently gaining traction in Kaggle competitions due to its speed. LightGBM allows us to adjust many model hyperparameters. The most important ones that are common to both RFs and GBDTs are n\_estimators, num\_leaves, feature\_fraction, and bagging\_fraction. In addition, for GBDTs, we also have the hyperparameters learning\_rate, lambda\_l1, and lambda\_l2, where the latter two hyperparameters control $L_1$ and $L_2$ regularization of the weights in a boosted model.

We used the python module ``hyperopt'' to help us perform efficient hyperparameter optimization (HO). We created an HO pipeline by combining LightGBM, Sci-kitLearn, and hyperopt as follows: We implemented RFs and GBDTs using the Sci-KitLearn interface provided by LightGBM. We used Sci-KitLearn to implement 3-fold cross-validation (CV). We then defined a loss function that is the mean of validation scores over the three folds. The goal of HO is then to find the hyperparameter combination that minimizes the loss within a predefined set of hyperparameter combinations or ``hyperspace''.

To make HO efficient, we used the Tree-Structure Parzen Estimation (TPE) algorithm. This algorithm works as follows: We first define a prior distribution over hyperspace; for our experiments, we stuck to a uniform prior. The algorithm then draws a random sample of hyperparameter combinations under the prior and evaluates the model and the 3-fold CV loss. It combines these observations with the prior through Bayes' Theorem to produce an updated posterior distribution over hyperspace. Sampling continues in this way to update the posterior iteratively. After a maximum number of iterations is reached, the most probable hyperparameter combination is returned.

After tuning our models, we make predictions on the test set and submit the predictions to Kaggle. Kaggle returns two test scores-- a public leaderboard (LB) and private LB. The public LB is the mean absolute error (MAE) in prediction on x\_test\_2016, and the private LB is the MAE on x\_test\_2017. We regard the private LB as the primary measurement of our success.


\section{Experimental results}

We implemented GBDT and RF models for various hyperparameter combinations and under various training conditions to obtain the best private LB. We present the results of seven such models. Table \ref{table-param} summarizes the hyperparameters used for each model. Some of the models have additional differences besides hyperparameters; therefore, we proceed to describe each model in more detail.

\begin{table}
\centering
\caption{\label{table-params} Set of hyperparameters used in each model. GBDT1 is the only model that uses different hyperparameters on the 2016 and 2017 data sets.}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline
\quad & n\_estimators & num\_leaves & feature\_fraction & bagging\_fraction & bagging\_freq & learning\_rate & lambda\_l1 & lambda\_l2 \\ \hline
GBDT0 & 200 & 60 & 0.5 & 1 & 0 & 0.002 & 0 & 0 \\
GBDT1 (2016) & 500 & 128 & 0.4 & 1 & 0 & 0.003 & 0 & 0 \\
GBDT1 (2017) & 173 & 128 & 0.3 & 1 & 0 & 0.01 & 0 & 0 \\
GBDT2 & 500 & 128 & 0.4 & 1 & 0 & 0.003 & 0 & 0 \\
GBDT3 & 500 & 128 & 0.4 & 1 & 0 & 0.003 & 0 & 0 \\
GBDT4 & 500 & 64 & 0.5 & 1 & 0 & 0.002 & 0 & 0 \\
GBDT5 & 500 & 64 & 0.5 & 0.8 & 10 & 0.002 & 0.001 & 0.01 \\
RF1 & 200 & 128 & 0.32 & 0.5 & 1 & 0.002 & * & * \\
\hline
\end{tabular}
\end{table}

GBDT0 is our base model that we borrowed from \cite{rtlatimer}. It is fundamentally different from the other six models as it was trained on a data set that was preprocessed somewhat differently than what we describe in Section \ref{section-preprocess}. In particular, ``finishedsquarefeet15'', ``rawcensustractandblock'', and ``calculatedbathnbr'' are not dropped; ``threequarterbathnbr'', and ``regionidcity'', ``regionidcounty'' are dropped; and ``regionidzip'' was imputed with its mode. Moreover, a single model was trained on 2016 data, and the predictions made on 2016 test data by the model were simply copied to obtain predictions on the 2017 test data. GBDT0 was originally implemented only to test our code, but we present it here due to its strong performance. The public and private LBs of the model are listed in Table \ref{table-scores}.

GBDT1 is the first model that we trained on (x\_train\_2016, y\_train\_2016) and (x\_train\_2017, y\_train\_2017) data as defined in Section \ref{section-transactions}. We implemented two separate models on 2016 and 2017 data. We performed HO for both models using the TPE algorithm and the mean 3-fold CV score as the loss. Table \ref{table-ho} lists the hyperparameter grids that were used for both the 2016 and 2017 models. We also indirectly optimized n\_estimators-- the hyperparameter that determines the number of boosts; we set n\_estimators to 500 but also implemented early stopping: At each boosting iteration of a GBDT as it is being trained on 2 of the 3 folds of the training data set, a validation score is calculated on the third fold. If this validation score does not improve in 5 consecutive boosting iterations, then boosting is terminated. The number of boosts that resulted in the best validation score up to that point is then recorded. The best number of boosts averaged over all three folds is taken to be the optimized value of the hyperparameter n\_estimators. We used the 2016 model to make predictions on x\_test\_2016 and the 2017 model to make predictions on x\_test\_2017; thus, we treated the 2016 and 2017 tasks as completely independent. Table \ref{table-scores} lists the CV score of both models as well as the public and private LBs. The private LB is significantly worse than that of GBDT0.

GBDT2 improves upon GBDT1 by using the hyperparameters optimized on (x\_train\_2016, y\_train\_2016) for both the 2016 model and 2017 model; i.e., we transfer the hyperparameters of the 2016 model to the 2017 model. Then as before, we train the two models separately on 2016 and 2017 training data and use each model to make predictions on the test data of its respective year.

GBDT3 is identical to GBDT2 but is applied to the PCA-compressed data (see Section \ref{section-pca}). PCA led to a poorer private LB.

GBDT4 improves upon GBDT2 by replacing the optimized hyperparameters with hand-picked ones. The hand-picked hyperparameters were selected to be close to the ones used in GBDT0. As in GBDT2, the same hyperparameters were used for both the 2016 and 2017 models.

GBDT5 improves upon GBDT4 through bagging and $L_1$ and $L_2$ regularization; these adjustments help to combat overfitting. More specifically, we took GBDT4 and performed HO on (x\_train\_2016, y\_train\_2016) with respect to the three hyperparameters that control bagging and regularization (see Table \ref{table-ho}). We used the optimized hyperparameters for both the 2016 model and 2017 model.

RF1 is an RF with hyperparameters optimized on 2016 training data (see Table \ref{table-ho}). The same hyperparameters were used for both 2016 and 2017 models. The mean CV score is reported along with the LBs (see Table \ref{table-score}). Out of all models trained on data preprocessed as described in Section \ref{section-preprocessed} (i.e., all models except GBDT0), RF1 achieves the best private LB. It is worth noting, however, that the improved score of RF1 comes at a significant cost in computation time; during HO for GBDT1 and GBDT5, each evaluation (for each hyperparameter combination) took about 5-7 seconds to run. In contrast, each evaluation during HO of RF1 took over 7 hours.

\begin{table}
\centering
\caption{\label{table-ho} hyperspaces that were searched during HO of each model.}
\begin{tabular}{|c|c|c|} \hline
Model & Hyperparameters & Grid \\ \hline
GBDT1 & num\_leaves, feature\_fraction, learning\_rate & $[32, 64, 128] \times [0.3, 0.4, 0.5, 0.6, 0.7] \times [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]$ \\
GBDT5 & bagging\_fraction, lambda\_l1, lambda\_l2 & $[0.2, 0.4, 0.6, 0.8, 1] \times [0, 0.001, 0.01] \times [0, 0.001, 0.01]$ \\
RF1 & n\_estimators, num\_leaves, feature\_fraction, bagging\_fraction & $[50, 100, 150, 200] \times [32, 64, 128] \times [0.8, 0.16, 0.32, 0.64] \times [0.2, 0.3, 0.4, 0.5]$ \\
\hline
\end{tabular}
\end{table}

\begin{table}
\centering
\caption{\label{table-scores} Mean CV, public LB, and private LB scores for each model. CV score is the mean of the validation over three folds. Since the hyperparameters of GBDT1 were optimized separately for 2016 and 2017, then two CV scores are reported-- one for each year.}
\begin{tabular}{|c|c|c|c|} \hline
Model & CV & Public LB & Private LB \\ \hline
GBDT0 & \quad & 0.0651179 & 0.0759204 \\
GBDT1 & \begin{tabular}{@{}c@{}} 0.0679511\\ 0.0703069\end{tabular} & 0.0647788 & 0.0763565 \\
GBDT2 & \quad & 0.0647788 & 0.0762839 \\
GBDT3 & \quad & 0.0648313 & 0.0763703 \\
GBDT4 & \quad & 0.0647504 & 0.0761217 \\
GBDT5 & 0.0679622 & 0.0647539 & 0.0761002 \\
RF1 & 0.0680366 & 0.0647544 & 0.0760869 \\
\hline
\end{tabular}
\end{table}

Both GBDT and RF models provide a natural measure of the ``importance'' of a feature. At each node of a decision tree, a feature and threshold are selected such that splitting the data with respect to the selected feature and threshold produces a maximal gain in ``purity''-- some measure of the homogenity of the target in each child subset. Features that are responsible for many splits and high gains in GBDT and RF models are then taken to be more important. Table \ref{table-importance} ranks the property features according to both the GBDT5 and RF1 models. The most important features according to both models appear to be the ones pertaining to taxes, overall area, and location. Some features have 0 importance, meaning that dropping them from the data would have no impact on performance; these features appear to be the ones that were highly imbalanced but that we imputed anyways.

\begin{table}
\centering
\caption{\label{table-importances} Importance of each feature as determined by the GBDT5 and RF1 models, where importance of a feature is calculated as the sum of decreases in loss over all splits of the feature in the GBDT or RF model. Features are sorted in order of descending importance.}
\begin{tabular}{|c|c|c|c|c|} \hline
Rank & Feature (GBDT5) & Importance (GBDT5) & Feature (RF1) & Importance (RF1) \\ \hline
1 & taxpercentage & 1628.243 & taxpercentage & 426.52 \\
2 & yearbuilt & 699.807 & calculatedfinishedsquarefeet & 219.81 \\
3 & calculatedfinishedsquarefeet & 685.086 & finishedsquarefeet50 & 210.467 \\
4 & structuretaxvaluedollarcnt & 671.976 & latitude & 151.716 \\
5 & regionidzip & 628.209 & structuretaxvaluedollarcnt & 149.174 \\
6 & taxvaluedollarcnt & 612.219 & landtaxvaluedollarcnt & 148.815 \\
7 & finishedsquarefeet50 & 573.487 & yearbuilt & 147.239 \\
8 & landtaxvaluedollarcnt & 524.328 & taxvaluedollarcnt & 142.675 \\
9 & lotsizesquarefeet & 477.144 & longitude & 132.993 \\
10 & latitude & 471.18 & regionidzip & 109.769 \\
\vdots & \vdots \\
35 & fireplacecnt & 2.399 & yardbuildingsqft17 & 0.531 \\
36 & yardbuildingsqft17 & 1.638 & yardbuildingsqft26 & 0.0 \\
37 & decktypeid & 0.0 & decktypeid & 0.0 \\
38 & yardbuildingsqft26 & 0.0 & pooltypeid2 & 0.0 \\
39 & basementsqft & 0.0 & basementsqft & 0.0 \\
\hline
\end{tabular}
\end{table}

\section{Conclusions}

***

\bibliographystyle{plain}
\bibliography{references}

\end{document}